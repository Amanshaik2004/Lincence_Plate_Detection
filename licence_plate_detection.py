# -*- coding: utf-8 -*-
"""Licence_Plate_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FRpCItLOpgnwaiCqf8b9lP39FMjw7nbj
"""

!pip install pytesseract

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models
import pytesseract
import torch
import pandas as pd
import warnings
import zipfile
from google.colab import drive
warnings.filterwarnings('ignore')

drive.mount('/content/drive')

# Step 2: Define paths to dataset in Google Drive (adjust the folder path as needed)
DRIVE_PATH = '/content/drive/MyDrive/LicensePlateDataset'
TRAIN_SET_1_ZIP = os.path.join(DRIVE_PATH, 'Licplatedetection_train.zip')
TRAIN_SET_2_ZIP = os.path.join(DRIVE_PATH, 'Licplaterecognition_train.zip')
TEST_SET_ZIP = os.path.join(DRIVE_PATH, 'test.zip')

# Step 3: Create directories to extract the ZIP files
os.makedirs('/content/Licplatedetection_train', exist_ok=True)
os.makedirs('/content/Licplaterecognition_train', exist_ok=True)
os.makedirs('/content/test', exist_ok=True)

# Step 4: Extract the ZIP files
with zipfile.ZipFile(TRAIN_SET_1_ZIP, 'r') as zip_ref:
    zip_ref.extractall('/content/Licplatedetection_train')

with zipfile.ZipFile(TRAIN_SET_2_ZIP, 'r') as zip_ref:
    zip_ref.extractall('/content/Licplaterecognition_train')

with zipfile.ZipFile(TEST_SET_ZIP, 'r') as zip_ref:
    zip_ref.extractall('/content/test')

# Step 5: Define paths to the extracted datasets
TRAIN_SET_1_PATH = '/content/Licplatedetection_train/license_plates_detection_train']

TRAIN_SET_2_PATH = '/content/Licplaterecognition_train/license_plates_recognition_train'
TEST_SET_PATH = '/content/test/test'

# Step 6: Load and preprocess Training Set 1 (vehicle images with bounding boxes)
train_set_1_csv = pd.read_csv(os.path.join(TRAIN_SET_1_PATH, "/content/Licplatesdetection_train.csv"))
train_set_1_images = []
train_set_1_bboxes = []

print(train_set_1_csv.head())

print(train_set_1_csv.info())

print(train_set_1_csv.describe())

print(train_set_1_images[0])

for index, row in train_set_1_csv.iterrows():
    # Changed 'img_id' to 'image_name' based on the full code in context
    img_path = os.path.join(TRAIN_SET_1_PATH, row['img_id'])
    img = cv2.imread(img_path)
    # Add a check to see if the image was loaded successfully
    if img is None:
        print(f"Warning: Could not load image from path: {img_path}")
        continue # Skip to the next iteration if image loading failed

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (416, 416))  # Resize for YOLOv5
    img = img / 255.0  # Normalize to [0,1]
    train_set_1_images.append(img)
    bbox = [row['ymin'], row['xmin'], row['ymax'], row['xmax']]
    train_set_1_bboxes.append(bbox)

train_set_1_images = np.array(train_set_1_images)
train_set_1_bboxes = np.array(train_set_1_bboxes)

#step 7: Load and preprocess Training Set 2 (license plate images with text)
train_set_2_csv = pd.read_csv(os.path.join(TRAIN_SET_2_PATH, "/content/Licplatesrecognition_train.csv"))
train_set_2_images = []
train_set_2_texts = []

for index, row in train_set_2_csv.iterrows():
    img_path = os.path.join(TRAIN_SET_2_PATH, row['img_id'])
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    img = cv2.GaussianBlur(img, (3, 3), 0)
    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    img = cv2.resize(img, (128, 32))  # Resize for OCR
    img = img / 255.0  # Normalize to [0,1]
    img = np.expand_dims(img, axis=-1)  # Add channel dimension
    train_set_2_images.append(img)
    train_set_2_texts.append(row['text'])

train_set_2_images = np.array(train_set_2_images)

print(train_set_2_images[0])

# Step 8: Load Test Set (vehicle images)
test_images = []
test_image_names = []
for img_name in os.listdir(TEST_SET_PATH):
    if img_name.endswith(('.jpg', '.png')):
        img_path = os.path.join(TEST_SET_PATH, img_name)
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (416, 416))  # Resize for YOLOv5
        img = img / 255.0  # Normalize to [0,1]
        test_images.append(img)
        test_image_names.append(img_name)

test_images = np.array(test_images)

yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Step 10: Build CNN-LSTM model for character recognition
input_shape = (32, 128, 1)  # Height, width, channels
inputs = layers.Input(shape=input_shape)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
x = layers.MaxPooling2D((2, 2))(x)
x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2))(x)
x = layers.Reshape((-1, x.shape[-1]))(x)
x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
outputs = layers.Dense(37, activation='softmax')(x)  # 36 chars (0-9, A-Z) + 1 for CTC blank
ocr_model = models.Model(inputs, outputs)

# Compile the model with CTC loss
ocr_model.compile(optimizer='adam', loss=lambda y_true, y_pred: tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length))

# Step 11: Prepare Training Set 2

char_map = {c: i for i, c in enumerate('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')}
max_len = 10
train_set_2_labels = np.zeros((len(train_set_2_texts), max_len), dtype=np.int32)
input_length = np.ones((len(train_set_2_texts), 1), dtype=np.int32) * 256
label_length = np.zeros((len(train_set_2_texts), 1), dtype=np.int32)

# Validate and prepare labels
for i in range(len(train_set_2_texts)):
    text = train_set_2_texts[i]
    # Ensure the text only contains valid characters
    text = ''.join([c for c in text.upper() if c in char_map])
    for j in range(min(len(text), max_len)):
        train_set_2_labels[i, j] = char_map.get(text[j], 0)
        # Double-check that the label index is within bounds (0 to 35)
        if train_set_2_labels[i, j] >= 36:  # 36 is the number of characters (0-9, A-Z)
            train_set_2_labels[i, j] = 0  # Replace invalid indices with 0
    label_length[i] = min(len(text), max_len)

# Step: Create a custom model to handle CTC loss properly
# Define inputs for the model
input_shape = (32, 128, 1)  # Height, width, channels
inputs = layers.Input(shape=input_shape, name='image_input')
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
x = layers.MaxPooling2D((2, 2))(x)
x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2))(x)
x = layers.Reshape((-1, x.shape[-1]))(x)
x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
outputs = layers.Dense(37, activation='softmax', name='output')(x)  # 36 chars (0-9, A-Z) + 1 for CTC blank

# Define additional inputs for input_length and label_length
input_length_input = layers.Input(shape=(1,), dtype='int32', name='input_length')
label_length_input = layers.Input(shape=(1,), dtype='int32', name='label_length')
labels_input = layers.Input(shape=(max_len,), dtype='int32', name='labels')

# Define a custom loss layer for CTC
def ctc_lambda_func(args):
    y_pred, labels, input_length, label_length = args
    return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)

# Add the CTC loss as a Lambda layer
loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc_loss')([outputs, labels_input, input_length_input, label_length_input])

# Create the model with multiple inputs and outputs
ocr_model = models.Model(
    inputs=[inputs, labels_input, input_length_input, label_length_input],
    outputs=[loss_out]
)

# Compile the model with a dummy loss (since the loss is already computed in the model)
ocr_model.compile(optimizer='adam', loss=lambda y_true, y_pred: y_pred)

# Step: Prepare training data as a dictionary of inputs
train_data = {
    'image_input': train_set_2_images,
    'labels': train_set_2_labels,
    'input_length': input_length,
    'label_length': label_length
}

# Dummy target data (not used in loss computation, but required by fit)
train_targets = np.zeros((len(train_set_2_texts), 1))

# Step: Train the CNN-LSTM model
ocr_model.fit(
    train_data,
    train_targets,
    epochs=50,
    batch_size=32,
    validation_split=0.2
)

# Step: Create a prediction model (without the CTC loss layer) for inference
prediction_model = models.Model(inputs=inputs, outputs=outputs)

# Step: Test pipeline (detect and recognize license plates)
results = []
for i in range(len(test_images)):
    img = test_images[i]
    img_name = test_image_names[i]

    # Detect license plate using YOLOv5
    yolo_results = yolo_model(img)
    bboxes = []
    for result in yolo_results.xyxy[0]:
        if result[-1] == 0:  # Assuming class 0 is license plate
            bbox = result[:4].cpu().numpy()  # ymin, xmin, ymax, xmax
            bboxes.append(bbox)

    if bboxes:
        bbox = bboxes[0]  # Assume one license plate per image
        ymin, xmin, ymax, xmax = map(int, bbox)

        # Load original image to crop the license plate
        orig_img = cv2.imread(os.path.join(TEST_SET_PATH, img_name))
        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
        plate_img = orig_img[ymin:ymax, xmin:xmax]

        # Preprocess the cropped license plate for OCR
        plate_img = cv2.cvtColor(plate_img, cv2.COLOR_RGB2GRAY)
        plate_img = cv2.GaussianBlur(plate_img, (3, 3), 0)
        _, plate_img = cv2.threshold(plate_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        plate_img = cv2.resize(plate_img, (128, 32))
        plate_img = plate_img / 255.0
        plate_img = np.expand_dims(plate_img, axis=-1)
        plate_img = np.expand_dims(plate_img, axis=0)

        # Recognize text using the prediction model
        pred = prediction_model.predict(plate_img)
        decoded, _ = tf.keras.backend.ctc_decode(pred, input_length=np.ones(1) * pred.shape[1])
        recognized_text = ''
        for idx in decoded[0].numpy()[0]:
            if idx == -1 or idx == 0:
                continue
            recognized_text += list(char_map.keys())[list(char_map.values()).index(idx)]

        # Visualize the result
        cv2.rectangle(orig_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
        cv2.putText(orig_img, recognized_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        plt.figure(figsize=(10, 6))
        plt.imshow(orig_img)
        plt.axis('off')
        plt.show()

        results.append({'image_name': img_name, 'text': recognized_text})

# Step: Save predictions to Google Drive
results_df = pd.DataFrame(results)
results_df.to_csv('/content/drive/MyDrive/LicensePlateDataset/test_predictions.csv', index=False)

# Step: Visualize the distribution of recognized text lengths
text_lengths = [len(result['text']) for result in results]
plt.figure(figsize=(10, 6))
sns.countplot(x=text_lengths)
plt.title('Distribution of Recognized Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Count')
plt.show()

results = []
for i in range(len(test_images)):
    img = test_images[i]
    img_name = test_image_names[i]

    # Detect license plate using YOLOv5
    yolo_results = yolo_model(img)
    bboxes = []
    for result in yolo_results.xyxy[0]:
        if result[-1] == 0:  # Assuming class 0 is license plate
            bbox = result[:4].cpu().numpy()  # ymin, xmin, ymax, xmax
            bboxes.append(bbox)

    if bboxes:
        bbox = bboxes[0]  # Assume one license plate per image
        ymin, xmin, ymax, xmax = map(int, bbox)

        # Load original image to crop the license plate
        orig_img = cv2.imread(os.path.join(TEST_SET_PATH, img_name))
        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
        plate_img = orig_img[ymin:ymax, xmin:xmax]

        # Preprocess the cropped license plate for OCR
        plate_img = cv2.cvtColor(plate_img, cv2.COLOR_RGB2GRAY)
        plate_img = cv2.GaussianBlur(plate_img, (3, 3), 0)
        _, plate_img = cv2.threshold(plate_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        plate_img = cv2.resize(plate_img, (128, 32))
        plate_img = plate_img / 255.0
        plate_img = np.expand_dims(plate_img, axis=-1)
        plate_img = np.expand_dims(plate_img, axis=0)

        # Recognize text using CNN-LSTM
        pred = ocr_model.predict(plate_img)
        decoded, _ = tf.keras.backend.ctc_decode(pred, input_length=np.ones(1) * pred.shape[1])
        recognized_text = ''
        for idx in decoded[0].numpy()[0]:
            if idx == -1 or idx == 0:
                continue
            recognized_text += list(char_map.keys())[list(char_map.values()).index(idx)]

        # Visualize the result
        cv2.rectangle(orig_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
        cv2.putText(orig_img, recognized_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        plt.figure(figsize=(10, 6))
        plt.imshow(orig_img)
        plt.axis('off')
        plt.show()

        results.append({'image_name': img_name, 'text': recognized_text})

results_df = pd.DataFrame(results)
results_df.to_csv('/content/drive/MyDrive/LicensePlateDataset/test_predictions.csv', index=False)

# Step 15: Visualize the distribution of recognized text lengths
text_lengths = [len(result['text']) for result in results]
plt.figure(figsize=(10, 6))
sns.countplot(x=text_lengths)
plt.title('Distribution of Recognized Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Count')
plt.show()

from google.colab import drive
drive.mount('/content/drive')